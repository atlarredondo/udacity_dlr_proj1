{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f978c1",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "\n",
    "The agent for this project was created by reusing the model and agent files from the DQN project. Some modifications were made in order to adapt the state/action space and improve the overall performance. \n",
    "\n",
    "First the agent used the same setup as the DQN project , reusing the Agent and Replay Buffer classes in `agent.py`.\n",
    "This version of the agent reached a maximum score of 16 and plateaued after ~600 episodes with an average score of ~15.5. \n",
    "\n",
    "![alt text](vanilla.png \"Final Double DQN Performance\")\n",
    "\n",
    "Even after adjusting the epsilon decay values the agent's performance remained low while  plateauing on later episodes.\n",
    "\n",
    "\n",
    "## Improvements\n",
    "I believed that the agent's bad performance was caused by remaining on the same strategy and not learning from the environment anymore. This might've been the result of overestimating q values. \n",
    "\n",
    "To resolve this issue the Agent was modified to instead use a Double DQN model when learning. \n",
    "The Double DQN network used the same set of weights used to \n",
    "The target Q values were changed to first extract the action values for the next states and use them to get the Q values of the Q local network. See agent.py lines 91-98.\n",
    "\n",
    "This agent learn at a slower pace when compared to the vanilla network but did not plateau after more than 1000 + episodes and reached a maximum average score of ~17\n",
    "\n",
    "The next improvement I did was to adjust the epsilon decay and epsilon end parameters to adjust the agent pace of learning and exploration.\n",
    "After several parameter changes the ones that worked the best were 0.005 for eps_end and 0.994 for epsilon decay. The agent works best when the decay is faster and the final epsilon value remains low.\n",
    "\n",
    "![alt text](double_dqn_0.005_994.png \"Final Double DQN Performance\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe43bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
