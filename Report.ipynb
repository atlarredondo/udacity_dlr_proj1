{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f978c1",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "\n",
    "The agent for this project was created by reusing the model and agent files from the DQN project. Some modifications were made in order to adapt the state/action space and improve the overall performance. \n",
    "\n",
    "First the agent used the same setup as the DQN project , reusing the Agent and Replay Buffer classes in `agent.py`.\n",
    "This version of the agent reached a maximum score of 16 and plateaued after ~600 episodes with an average score of ~15.5. \n",
    "\n",
    " ```\n",
    " Episode 100\tAverage Score: 1.00\n",
    " Episode 200\tAverage Score: 4.52\n",
    " Episode 300\tAverage Score: 8.26\n",
    " Episode 400\tAverage Score: 9.80\n",
    " Episode 500\tAverage Score: 11.98\n",
    " Episode 600\tAverage Score: 13.07\n",
    " Episode 700\tAverage Score: 14.77\n",
    " Episode 800\tAverage Score: 15.38\n",
    " Episode 900\tAverage Score: 15.92\n",
    " Episode 1000\tAverage Score: 16.17\n",
    " Episode 1100\tAverage Score: 15.47\n",
    " Episode 1200\tAverage Score: 15.76\n",
    " Episode 1300\tAverage Score: 15.48\n",
    " Episode 1400\tAverage Score: 15.99\n",
    " Episode 1500\tAverage Score: 15.31\n",
    " Episode 1600\tAverage Score: 15.83\n",
    " Episode 1700\tAverage Score: 16.09\n",
    " Episode 1800\tAverage Score: 15.05\n",
    " ```\n",
    "\n",
    "![alt text](vanilla.png \"Final Double DQN Performance\")\n",
    "\n",
    "Even after adjusting the epsilon decay values the agent's performance remained low while  plateauing on later episodes.\n",
    "\n",
    "\n",
    "## Improvements\n",
    "I believed that the agent's bad performance was caused by remaining on the same strategy and not learning from the environment anymore. This might've been the result of overestimating q values. \n",
    "\n",
    "To resolve this issue the Agent was modified to instead use a Double DQN model when learning. \n",
    "The Double DQN network used the same set of weights used to \n",
    "The target Q values were changed to first extract the action values for the next states and use them to get the Q values of the Q local network. See agent.py lines 91-98.\n",
    "\n",
    "This agent learn at a slower pace when compared to the vanilla network but did not plateau after more than 1000 + episodes and reached a maximum average score of ~17\n",
    "\n",
    "The next improvement I did was to adjust the epsilon decay and epsilon end parameters to adjust the agent pace of learning and exploration.\n",
    "After several parameter changes the ones that worked the best were 0.005 for eps_end and 0.994 for epsilon decay. The agent works best when the decay is faster and the final epsilon value remains low.\n",
    "\n",
    "![alt text](double_dqn_0.005_994.png \"Final Double DQN Performance\")\n",
    "\n",
    "\n",
    "```\n",
    "Episode 100\tAverage Score: 0.56\n",
    "Episode 200\tAverage Score: 4.62\n",
    "Episode 300\tAverage Score: 9.31\n",
    "Episode 400\tAverage Score: 12.14\n",
    "Episode 500\tAverage Score: 13.90\n",
    "Episode 600\tAverage Score: 15.01\n",
    "Episode 700\tAverage Score: 15.58\n",
    "Episode 800\tAverage Score: 16.56\n",
    "Episode 900\tAverage Score: 16.47\n",
    "Episode 915\tAverage Score: 17.02\n",
    "Episode 917\tAverage Score: 17.11\n",
    "Episode 920\tAverage Score: 17.15\n",
    "Episode 921\tAverage Score: 17.18\n",
    "Episode 923\tAverage Score: 17.22\n",
    "Episode 924\tAverage Score: 17.23\n",
    "Episode 925\tAverage Score: 17.30\n",
    "Episode 926\tAverage Score: 17.31\n",
    "Episode 927\tAverage Score: 17.34\n",
    "Episode 928\tAverage Score: 17.36\n",
    "Episode 929\tAverage Score: 17.37\n",
    "Episode 930\tAverage Score: 17.42\n",
    "Episode 931\tAverage Score: 17.44\n",
    "Episode 932\tAverage Score: 17.48\n",
    "Episode 966\tAverage Score: 17.48\n",
    "Episode 967\tAverage Score: 17.51\n",
    "Episode 969\t\tAverage Score: 17.51\n",
    "Episode 970\t    Average Score: 17.55\n",
    "Episode 1000\tAverage Score: 16.93\n",
    "Episode 1100\tAverage Score: 15.74\n",
    "Episode 1200\tAverage Score: 16.25\n",
    "Episode 1300\tAverage Score: 15.58\n",
    "Episode 1400\tAverage Score: 16.09\n",
    "Episode 1500\tAverage Score: 16.37\n",
    "Episode 1600\tAverage Score: 15.26\n",
    "Episode 1700\tAverage Score: 16.16\n",
    "Episode 1800\tAverage Score: 15.58\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe43bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
